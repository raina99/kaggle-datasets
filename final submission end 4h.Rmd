---
output:
  html_document: default
  word_document: default
---
---
title: "R Notebook"
output: html_notebook
----

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

 NYC TAXI TRIP DATASET FROM KAGGLE
   PROBLEM STATEMENT  - PREDICT TRIP DURATION
 DATASET DETAILS 
 Data fields
1 id - a unique identifier for each trip
2 vendor_id - a code indicating the provider associated with the trip record
3 pickup_datetime - date and time when the meter was engaged
4 dropoff_datetime - date and time when the meter was disengaged
5 passenger_count - the number of passengers in the vehicle (driver entered value)
6 pickup_longitude - the longitude where the meter was engaged
7 pickup_latitude - the latitude where the meter was engaged
8 dropoff_longitude - the longitude where the meter was disengaged
9 dropoff_latitude - the latitude where the meter was disengaged
10 store_and_fwd_flag - This flag indicates whether the trip record was held in vehicle
memory before sending to the vendor because the vehicle did not have 
 a connection to the server - Y=store and forward; N=not a store and forward trip
11 trip_duration - duration of the trip in secondS


## Note1 - The inferences obtained from data are written after executing code.

## note2 -This notebook is an approach to analyze dataset and develop insights and has not taken or copied code majorly from the famous kernels published for this dataset on kaggle.

## Note 3 - Majority of time is spend on analyzing data and coming up with insightsand rest time left is utilized for building a competitive model which performs efficiently and judged on the basis of evaluation metric  score generated by model on kaggle.

# Lets get started with the ride

```{r}
## Loading libraries
library(geosphere)
library(RColorBrewer)
library(ggmap)
library(data.table)
library(ggplot2)
library(dplyr)
library(lubridate)
library(fasttime)
library(readr)
library(magrittr)
library(h2o)
library(randomForest)
library(gbm)
library(xgboost)
library(plyr)
library(dplyr)
```

```{r}
# Reading datasets
train = fread("train.csv", na.strings = c("",NA))
test = fread("test.csv",  na.strings = c("",NA))
sample_submission = read.csv("sample_submission.csv")

```
```{r}


### Data exploration 
glimpse(train)

```

```{r}
summary(train)

```

```{r}
glimpse(test)

```

```{r}
summary(test)

```

 Median of passenger count of train and test  being 1 and max being 9
 Median of trip duration  in train being 662 sec - 11 min
 There are only two vendor id - 1 and 2  two taxis 
 Flag-yes/no means that information of trip was sent to vendor  or not sent based  on possibility of  error due to connection

```{r}
# inspecting miising values 
sum(is.na(train))
sum(is.na(test))
```

 No missing data in train or test
```{r}
# let us build basic plots to understand data
# adding datetime and duration to test to make column same to combine
# using pipe (%>%) in r 
test <- test %>%
mutate(dropoff_datetime = 0,
trip_duration = 0) 

```



```{r error = TRUE}
## WORKING ON DATETIME AND CRETAING FEATURES OUT OF IT
         
train$pickup_datetime = ymd_hms(train$pickup_datetime)
train$dropoff_datetime = ymd_hms(train$dropoff_datetime)

```

```{r}
### 
#label - TRUE ensures that we get month and wdays in character not in numbers
## for train
train$month<- lubridate::month(train$pickup_datetime, label = TRUE) 
train$wday <- lubridate::wday (train$pickup_datetime, label = TRUE)
train$hour <- lubridate::hour(train$pickup_datetime)
train$date <- lubridate::day(train$pickup_datetime)
train$minutes <-lubridate:: minute(train$pickup_datetime)
```




```{r}
## for test
test$pickup_datetime = ymd_hms(test$pickup_datetime)
test$month<- lubridate::month(test$pickup_datetime, label = TRUE) 
test$wday <- lubridate::wday (test$pickup_datetime, label = TRUE)
test$hour <- lubridate::hour(test$pickup_datetime)
test$date <- lubridate::day(test$pickup_datetime)
test$minutes <-lubridate:: minute(test$pickup_datetime)
```


  UNIVARIATE ANALYSIS
 On train dataset
 Important variable being target variable that is trip duration 

```{r}
ggplot(data = train, mapping = aes(x = trip_duration))+
  geom_histogram(bins = 80, color = "green")+
  scale_x_log10()+
  scale_y_sqrt()

```
 AS it tells it looks like a normal distribution with some values at  extreme pinging

```{r}
ggplot(data = train, mapping = aes(x = passenger_count))+
  geom_bar( fill  = "blue")
```

 As  we see majority of times  passenger count is 1 and it reflects exact value(close to 10 lakh)
 values of passenger_ count are also there but they are not visible here
 So  in order to get full figures for all passenger_count

```{r}
table(train$passenger_count)
```

 Passenger count being 8 and 9 are quite low 

```{r}
boxplot(train$passenger_count, ylab = "passenger_count", main= "passenger_count,mean(magenta),median(red)")
abline(h = mean(train$passenger_count),col= "magenta")
abline(h = median(train$passenger_count, col = "red", lwd = 2))

```

 As we see how much data is skewed  as median is way too less than mean
 It is right skewed

```{r}
summary(train$passenger_count)
```

```{r}
## vendor_id
ggplot(data = train, mapping =aes(x = vendor_id ) )+
  geom_bar()
table(train$vendor_id) # gives exact numbers
##
```
 Maximum trips taken by  vendor 2 as compared to vendor 1


```{r}
ggplot(data = train, mapping =aes(x = store_and_fwd_flag ) )+
  geom_bar()

table(train$store_and_fwd_flag)
```


 Majority of trips information was not sent as so majority of trips had no connection
 To  find proprtion                        

```{r}
 as.matrix(prop.table(table(train$store_and_fwd_flag)))
```
 99 percent of trips had no information sent to the vendor




 Pickup_datetime
```{r}
## pickupdatetime
 ggplot(data = train, mapping =aes(x = pickup_datetime ) )+
   geom_histogram(fill = "blue", bins = 120)
 
```

 As we see there is dip in the frequency around ending january and starting of february
  The middle line between jan and apr corresponds to the 45 day from jan 
Half of middle line corresponds to 22 1/2 days from jan and there is dip around that place


Dropoff_datetime
```{r}
##  Drop_off datetime  
ggplot(data = train, mapping =aes(x = dropoff_datetime ) )+
geom_histogram(fill = "blue", bins = 120)

```


 As understandable  same pattern repeats in dropoff_datetime
 
 
 
 Month
 
```{r}
## month- PICKUPS
ggplot(data = train, mapping =aes(x = month ) )+
  geom_bar(fill = "green")

```

As we see third month shows the highest activity followed by a fourth month and first month the least 
 Exact values are as under
 
```{r}
table(train$month)
```




To understand better
```{r}
boxplot(train$wday, col = "red")
abline(h = mean(train$month), col = "magenta")
```
Does not have  any outliers and range of values from 2 to 6 constitutes IQR


WDAYS

```{r}
## wday- PICKUPS
 ggplot(data = train, mapping =aes(x = wday ) )+
   geom_bar(fill = "blue")
```
 Saturday has highest activity followed by sunday, others tuesday has lowest
 
 
 Exact figures
 
```{r}
table(train$wday)
```


```{r}
boxplot(train$wday, col = "red")
abline(h = mean(train$wday), col = "magenta")

```
Here mean and median  are some what close to each other slightly hinting at normalized curve
however it cant be always true 

## Minutes
```{r}
#minutes - PICKUPS
ggplot(data = train, mapping =aes(x = minutes ) )+
   geom_bar(fill = "blue")
```
Mostly follows a uniform  pattern  with a slight dip in between 25 minutes



```{r}
hist(train$minutes, breaks = 100, col= "grey", main = "Histogram of minutes with  mean(red)-median(green)", xlab = "Minutes")
abline(v = mean(train$minutes), col = "red") # mean shown by yellow line
abline(v = median(train$minutes), col = "green")# median  shown by green line
```

It is almost symetrical with  mean and median merging at a value of  30

DAYS

```{r}
## day - PICKUPS
ggplot(data = train, mapping =aes(x = date ) )+
  geom_bar(fill = "blue")
```
As it shows   that end of month has least activity so people travel less at the end of month 

 Exact vaues can be seen
```{r}
table(train$date)
 
```

Lets move to the next stage 

 MULTIVARIATE ANALYSIS
 
 

Lets create a subset df
```{r}
df = train[,c(6,7)]
names(df)  <- (c("lon","lat"))
glimpse(df)
```


 longitude  and latitude
 
```{r}
## visulaize data using latitude and longitude
 ggplot(data = df, mapping = aes(x=lon, y=lat))+
geom_jitter(size=0.6) +
  labs(x = "longitudes", y = "latitudes")

```

It gives us the general idea about the distribution of pickups
 
 
 
 Observing  new york coordinates and placing a limit 

```{r    error= TRUE}
min_lat <- 40.6
max_lat <- 40.9
min_long <- -74.05
max_long <- -73.7

ggplot(df, aes(x=lon, y=lat)) +
geom_jitter(size=0.6) +
scale_x_continuous(limits=c(min_long, max_long)) +
scale_y_continuous(limits=c(min_lat, max_lat))
```
Now it shows all the pickups points with their latitudes and longitudes

 In order to make visualization  more appealing we try following tweaks
 
```{r    error = TRUE}
ggplot(df, aes(x=lon, y=lat)) +
geom_jitter(size=0.6, color = "white") +
scale_x_continuous(limits=c(min_long, max_long)) +
scale_y_continuous(limits=c(min_lat, max_lat))+
theme_dark()
```


 It makes things a bit clearer but  a lot needs to be done
 In order to bring a slight clearity we will count the number of pickups
 for a particular latitudes and longitudes
 
 
 Due to computational deficinies  we can't calculate num of pickups from same location for all of data 
 so we will coinsider only first 1 k obervations
 
 
```{r}
df_pickups= df
df_pickups = as.data.frame(count(df_pickups, c("lon","lat")))
head(df_pickups)

```
 Third variable freq tells us number of pickups happening at 
the same exact longitude and latitude
 so we will plot only those location out of these values where num of pickups is only 1
  as these constitute the maximum
  
  
```{r     error = TRUE}
plot_1 <- ggplot(df_pickups %>% filter(freq == 1), aes(x=lon, y=lat, color=freq)) +
  geom_jitter(size=0.06) +
  scale_x_continuous(limits=c(min_long,max_long)) +
  scale_y_continuous(limits=c(min_lat,max_lat)) +
  theme_dark() +
  scale_color_gradient(low="#CCCCCC", high="#8E44AD", trans="log") +
  labs(title = "Map of NYC") +
  theme(legend.position="none") +
  coord_equal()
plot_1

```


```{r    error = TRUE}
plot_2 <- ggplot(df_pickups %>% filter(freq == 2), aes(x=lon, y=lat, color=freq)) +
  geom_jitter(size=0.06) +
  scale_x_continuous(limits=c(min_long,max_long)) +
  scale_y_continuous(limits=c(min_lat,max_lat)) +
  theme_dark() +
  scale_color_gradient(low="#CCCCCC", high="#8E44AD", trans="log") +
  labs(title = "Map of NYC") +
  theme(legend.position="none") +
  coord_equal()
plot_2

```




Passenger count >4 , lon, lat  and trip_duration


```{r     error = TRUE}
#Let use further combinations to plot different variables
plot4 <- ggplot(train %>% filter(passenger_count>4), aes(x=pickup_longitude, y=pickup_latitude, z=trip_duration)) +
  geom_point(size=0.6, color="#999999") +
  stat_summary_hex(fun = sum, bins=100, alpha=0.7) +
  scale_x_continuous(limits=c(min_long, max_long)) +
  scale_y_continuous(limits=c(min_lat, max_lat)) +
  theme_dark() +
  scale_fill_gradient(low="#CCCCCC", high="#27AE60", labels=seconds) +
  labs(title = "Total Trip duration by Pickup Location") +
  coord_equal()
plot4
```



Combination with vendor_id

```{r   error = TRUE}
## lets us try another combination with respect to vendor_id

plot5 <- ggplot(train %>% filter( passenger_count>4), aes(x=pickup_longitude,y=pickup_latitude, color=vendor_id)) +
  geom_jitter(size=0.06) +
  scale_x_continuous(limits=c(min_long,max_long)) +
  scale_y_continuous(limits=c(min_lat,max_lat)) +
  theme_dark() +
  scale_color_gradient(low="#CCCCCC", high="#8E44AD", trans="log") +
  labs(title = "Map of NYC") +
  theme(legend.position="none") +
  coord_equal()
plot5
```



 Lets move on to other variables
 
 
 MULTIVARIATE ANALYSIS
 
 
Arranging the dataset
```{r}
train  = arrange(train,month,date,wday,hour,minutes )

```

```{r}
ggplot(data = train, mapping =aes(x = vendor_id, y = store_and_fwd_flag))+
geom_jitter()

```
It clearly shows that all trips with record sent(flag value = Y ) to server belonged to vendor 1
```{r}

ggplot(data = train, mapping =aes(x = vendor_id,y = passenger_count))+
geom_jitter()

```
It tells that distribution is more for seats 1 to 4 for vendor 1
 It tells that distribution is more for seats 1 to 6.25 for vendor 2
 Passenger count with high values(>6)  visible  only for vendor 2

```{r}
ggplot(data = train,mapping =aes(x = vendor_id,y = trip_duration))+
geom_jitter()
```


While the bulk of trips have generally same duration but vendor 1 has higher values of trip duration

```{r}
ggplot(data = train, mapping =aes(x = vendor_id, y = passenger_count, color = as.factor(store_and_fwd_flag )))+
geom_jitter()
  
```
The graph above highlights things more clearly

 Store flag(Y) is being sent by vendor1 for passenger count between  1 and 5

```{r}
ggplot(data = train, mapping =aes(x = vendor_id, fill = as.factor(store_and_fwd_flag )))+
geom_bar()
```


Adding passenger count


```{r}
ggplot(data = train, mapping =aes(x = vendor_id, fill = as.factor(store_and_fwd_flag )))+
geom_bar()+
facet_grid(~passenger_count)
```


It shows as majority of bookings at passenger_count at 1 and 2 
 Considerable Yes flag present for passenger with 1 and 2 persons



 DATE AND TIME analysis MULTIVARIATE



 MONTH
```{r}
ggplot(data = train, mapping =aes(x = month, fill = as.factor(vendor_id)))+
  geom_bar(position ="dodge")+
  labs(y = "count", x = "Months")
```

MARCH has highest activity in total
MAY comes in second number
 JAN has least activity

DAYS OF THE WEEK

```{r}
##  DAYS OF WEEK
ggplot(data = train, mapping =aes(x = wday, fill = as.factor(vendor_id)))+
geom_bar(position ="dodge")+
labs(y = "Total number of pickups", x = "Days of the week")
```


It tells the exact count 
VENDOR_ID 2 peaks consistently on all the days of the week
MONDAY has least activity considering both trip_vendor 1 and 2
FRIDAY has highest activity

HOURS OF THE DAY


```{r}
##  HOURS OF THE DAY
ggplot(data = train, mapping =aes(x = hour, fill = as.factor(vendor_id)))+
geom_bar()+
labs(y = "Total number of pickups", x = "Hours")
```


VENDOR_1 leads on all the hours in terms of pickups 
 Least activity around 4 and 5 am 
 Most activity around  18 and 19 hours- 6pm and 7 pm




 HOURS OF DAY AND MONTH
  
```{r    error = TRUE}
ggplot(data = train, mapping =aes(x = hour,fill = month))+
geom_bar(position = "dodge")+
facet_grid(~month, scales = "free")+
scale_fill_manual(labels= levels(month), values=c("red","blue","green","brown","pink","magenta"))+ 
labs(y = "Total number of pickups", x = "Hours")
```

MARCH   has highest activity in pickups in total  and it has peaks during peak hour 6pm and 7pm
 Dip across all months during 4 and 5 am in all months


HOURS AND WDAYS
```{r   error = TRUE}
#### HOURS OF THE DAY AND WDAYS

ggplot(data = train, mapping =aes(x = hour,fill= wday))+
geom_bar(position = "dodge")+
facet_grid(~wday, scales = "free")+
scale_fill_manual(labels=  levels(wday) ,values=c("red","blue","green","brown","pink","magenta","yellow"))+ 
labs(y = "Total number of pickups", x = "Hours")
```


Same graph plotted differently using color
```{r   error = TRUE}
# same graph plotted by using color
ggplot(data = train, mapping =aes(x = hour,color= wday))+
  geom_bar(position = "dodge")+
  facet_grid(~wday, scales = "free")+
  scale_fill_manual(labels= c("sun","mon","tue","wed","Thur","fri","sat") ,values=c("red","blue","green","brown","pink","magenta","yellow"))+ 
  labs(y = "Total number of pickups", x = "Hours")

```





HOURS OF THE DAY AND MINUTES IN THE HOUR

```{r error = TRUE}
#HOUR OF DAY AND MINUTES IN THE HOUR
ggplot(data = train, mapping =aes(x = minutes,fill=   as.factor(hour)))+
  geom_bar(position = "dodge")+
  facet_grid(~ hour, scales = "free")+
  labs(y = "Total number of pickups", x = "Minutes")

```

Initial minutes of hours such as 0,1,2,3,4,23 showing peak as compared to other minutes
End minutes of hours such as 5,6,7,8, 10,11,12,13 ,17,18
 for rest hours distribution is somewhat uniform
 most of the hours there is dip in middle minutes


 ### As it is seen during peak hours at 6 pm and 7 pm thursday and friday show maximum activity
 During  initial hours 0 and 1 sun and sat show the highest values as other days report less
  
 
 
 calculating distance between lat long coordinates between pickups and dropoff

 We use HAVERSHINE formula to calculate distance between points 
 we  manually confirmed the  output by the formula to ascertain that the dustance it gives is correct
 output -distance is in the form of metres


```{r}
train$distance<-distHaversine(train[,6:7],train[,8:9])
```


TRIP DURATION, VENDOR_ID, WDAY 

```{r}
##   TRIP DURATION, VENDOR_ID, WDAY 

train$median_duration = median(train$trip_duration)/60

```

TRIP DURATION, VENDOR_ID, WDAY
```{r error = TRUE}
plot8 <- train %>%
   group_by(wday,vendor_id) %>%
   summarise(median_duration = median(trip_duration)/60) %>%
  ggplot(aes(wday, median_duration, color = as.factor( vendor_id))) +
  geom_point(size = 4) +
  labs(x = "Day of the week", y = "Median trip duration [min]")
plot8
```

 
Medain trip duration for vendor 2 is higher on all days except on mon
 Medain trip duration is highest on thursday

```{r error = TRUE}






 p10 = train %>%
  ggplot(aes(passenger_count, trip_duration, color = as.factor(passenger_count))) +
  geom_boxplot() +
  scale_y_log10() +
  theme(legend.position = "none")+
  facet_wrap(~ vendor_id) +
  labs(y = "Trip duration [s]", x = "Number of passengers")
p10
```

It shows us the median values as for no passengers short trips
vendor 1 has high value lengthy trip as seen by outliers unlike vendor 2
 For passenger from 2 to 6 generally they have same IQR 
 IQR more consistaent for vendor 2 as compared to vendor 1

DENSITY PLOT
```{r error = TRUE}
##  DENSITY PLOT
train %>%
  ggplot(aes(trip_duration, fill =  as.factor(vendor_id))) +
  geom_density(position = "stack") +
  scale_x_log10()
## Vendor 1 has better density due to long trips as seen by outliers present  in boxplot earlier

```

Vendor 1 has better density due to long trips as seen by outliers present  in boxplot earlier


 FOR VENDOR 1 ONLY 


```{r error = TRUE}
p11 <- train %>%
  filter(vendor_id == 1) %>%
  ggplot(aes(passenger_count, trip_duration, color = as.factor( passenger_count))) +
  geom_boxplot() +
  scale_y_log10() +
  facet_wrap(~ store_and_fwd_flag) +
  labs(y = "Trip duration [s]", x = "Number of passengers") +
  ggtitle("Store_and_fwd_flag effectt")

p11

```
 For trips having Store_and_fwd_flag(Y)
 Short trips are far short 
longer trips are less

For trips having Store_and_fwd_flag(N)
 Long duration trips are motre due to values present
 IQR range is very consistent
```{r error = TRUE}
### DISTANCE(IN METRES ) BY HAVERSHINE FORMULA and  TRIP DURATION
ggplot(data = train, mapping = aes(x = distance, y = trip_duration))+
geom_jitter()+
scale_x_log10()+
scale_y_log10()+
labs(x = " Distance [m]", y = "Trip duration[s]")
```


 we have used havershine function to calculate distance 
The haversine formula determines the great-circle distance between two points 
on a sphere given their longitudes and latitudes. Important in navigation, 
it is a special case of a more general formula in spherical trigonometry
 geom_jitter gives us all the values even the overplotted ones unlike geom-point





 As  we see as distance increases the trip duration increases with other outliers present too
 These are certain values  that show some short trips with more trip duration
 There are values that show some long distance trips with least trip duration

```{r}
train$speed = train$distance/train$trip_duration
```

```{r error = TRUE}
ggplot(data = train, mapping =aes(x = speed, y = trip_duration))+
geom_jitter()+
scale_x_log10()+
scale_y_log10()
```
moderate speed has moderate trip duration



 Speed and DATE, MONTH,DAYS

```{r     error = TRUE}
p12 <- train %>%
  group_by(wday, vendor_id) %>%
  summarise(median_speed = median(speed)) %>%
  ggplot(aes(wday, median_speed, color =  as.factor(vendor_id))) +
  geom_point(size = 4) +
  facet_grid(~vendor_id)+
  labs(x = "Day of the week", y = "Median speed [m/s]")

  p12
```
 As evident in both cases sunday has high nedian speed and thus we can expect longer trips
 on sundays by both vendors

  
 ##  Median speed , hour, wday
  
  
```{r     error = TRUE}
 p13 <- train %>%
    group_by(hour,wday) %>%
    summarise(median_speed = median(speed)) %>%
    ggplot(aes(hour, median_speed, color =as.factor(wday))) +
    geom_jitter(size = 4) +
    facet_grid(~wday)+
    labs(x = "Hour", y = "Median speed [m/s]")
  
  p13  
```


Median speed is high for initial hours at 5am for all days 
 Median speed is low for  late hours at 6 pm and 7pm for all days
  
  
lets see if we can find names from longitudes and latitudes
 Due to compuattional deficiencies and time it will take we will consider only few hundred observations
  


## rest visualization and model of  portion in another notebook as it is heavy and taking time to load graphs

lets see if we can find names from longitudes and latitudes
 Due to compuattional deficiencies and time it will take we will consider only  500 latitudes and longitudes





```{r error = TRUE}
dm = train[1:500,]
dm$pickup_adress<- mapply(FUN = function(pickup_longitude,pickup_latitude) revgeocode(c(pickup_longitude,pickup_latitude)), dm$pickup_longitude, dm$pickup_latitude)
dm[5,20]

```


```{r error = TRUE}
dm[1,20]
dm$pickup_adress = as.character(dm$pickup_adress)
dm[1:5,c(20,6,7,8,9)]
## It provides us with the adress 
```


It provides us with the adress for each data latitude and longitude


## Text analysis 
 we can further use info to detect  high frequency pickup stops and also check their efficiency
 
 
```{r}
 library(tm)
corpus1 = Corpus(VectorSource(dm$pickup_adress))
corpus1 <- tm_map(corpus1, removePunctuation)
corpus1 <- tm_map(corpus1,content_transformer(tolower))
corpus1<- tm_map(corpus1,stripWhitespace) 
writeLines(as.character(corpus1[2]))
```

  We removed punctuation , whitespace 
 Also converted  corpus into lower case letters.
 
 we also got ouput for line no 2 
 
 It would make more sense if we remove obvious nin informative terms 
 such as ny new york and usa as they dont tell specific as all data is from new york
 
 
 
 
 
```{r}
corpus1<- tm_map(corpus1,removeWords,c("new york","ny","usa","na"))
writeLines(as.character(corpus1[2]))
```




It is more specific and tell us about the specific adresses so we now  use build wordcloud


```{r}
dt1 = DocumentTermMatrix(corpus1)
dt1_f = removeSparseTerms(dt1,0.99)## removing sparsity
dt1_f = as.matrix(dt1_f)
freq = colSums(as.matrix(dt1_f))
```


We have built a document term matrix to build a wordcloud

```{r}
library(wordcloud)
freq
```


Gives us the frequency of pickup adresses

```{r}

## threshold  frequency as 1
wordcloud(names(freq),freq, max.words = 200, min.freq = 1, scale=c(3, .8), colors=brewer.pal(8, "Dark2"))
```
```{r}
wordcloud(names(freq), freq, max.words = 200, min.freq = 10, scale=c(3, .8), colors=brewer.pal(8, "Dark2"))
```
```{r}
wordcloud(names(freq), freq, max.words = 200, min.freq = 12, scale=c(3, .8), colors=brewer.pal(8, "Dark2"))
#
```
```{r}
wordcloud(names(freq), freq, max.words = 200, min.freq = 16, scale=c(3, .8), colors=brewer.pal(8, "Dark2"))
```
```{r}
wordcloud(names(freq), freq, max.words = 200, min.freq = 18, scale=c(3, .8), colors=brewer.pal(8, "Dark2"))

```

```{r}
wordcloud(names(freq), freq, max.words = 200, min.freq = 20, scale=c(3, .8), colors=brewer.pal(8, "Dark2"))

```


```{r}
wordcloud(names(freq), freq, max.words = 200, min.freq = 25, scale=c(3, .8), colors=brewer.pal(8, "Dark2"))



```

Thus we easily check the frequency of stops from it

PICKUP STOPS 
most common word  comes to be "ave" and ceratin street numbers  like 10011,10003
 so we can get the most common terms and frequencies of all terms or words present in name of stops
 we can get the frequency of exact stops and thus even create a route from pickups and dropoff



###### Visualization complete for now ##############



### MODEL BUILDING #########


#### Model preparation 


```{r}
## Making columns of test dataset same as train


test$pickup_datetime = ymd_hms(test$pickup_datetime)
test$dropoff_datetime = ymd_hms(test$dropoff_datetime)

test$month <- lubridate::month(test$pickup_datetime, label = TRUE)
test$wday <- lubridate ::wday(test$pickup_datetime, label = TRUE)

test$hour <- lubridate:: hour(test$pickup_datetime)
test$date <- lubridate::day(test$pickup_datetime)
test$minutes <- lubridate::minute(test$pickup_datetime)

```

```{r}
test$distance<-distHaversine(test[,5:6],test[,7:8])
```


we can't get speed variable for test as we don't have trip_duration for test

```{r}

train[,c(4,8,9)] = NULL
test[,c(7,8)] = NULL
```




Removing certain columns such as dropoff_datetime,dropoff_latitude and dropoff_longitude

```{r}
# checking whether train and test have  same columns
names(train)
names(test)
```


```{r}
## checking same levels in train and test
unique(train$month)
unique(test$month)
```




```{r}
 #In order to check data type of train and test variables  we use glimpse
glimpse(train)
glimpse(test)
```



```{r}
## Converting  datatype from character to factor for store_and_fwd_flag  in train and test
train $store_and_fwd_flag = as.factor(train$store_and_fwd_flag)
test$store_and_fwd_flag = as.factor(test$store_and_fwd_flag)
```




In order to reduce computation time  we will apply machine algorithms using H2o
 Now applying machine learning model in H2O
H2O is an open source machine learning platform where companies
can build models on large data sets (no sampling needed) and achieve accurate predictions.



```{r}
h2o.init(nthreads = -1)
```




```{r}
## checking connection
h2o.init()
```

```{r}
# converting datetime variable to date format as h2o does not  understand POSIXct
class(train$pickup_datetime) 
train$pickup_datetime = as.Date(train$pickup_datetime)
test$pickup_datetime = as.Date(test$pickup_datetime)
train$month = as.factor(as.numeric(train$month))
train$wday =  as.factor(as.numeric(train$wday))
test$month = as.factor(as.numeric(test$month))
test$wday =  as.factor(as.numeric(test$wday))
```



 For evaluation metric being Root mean square logarithmic error(RMSLE) we need to change target variable

```{r}
train$trip_duration = log(train$trip_duration +1)
```

```{r  error = TRUE}
## data to h2otest
train2.h2o = as.h2o(train2)
test2.h2o = as.h2o(test2)
```

both the files added

After trying few iterations  (trying different parameters )and different algorithms  I found these two working for me


I have skipped some preprocessing done here and there

GBM  and random forest.





```{r   error = TRUE}
#dependent variable (trip_duration)
 y.dep3 <- 7

#independent variables (dropping ID variable)
 x.indep3 <- c(2:6,8:13)
```


```{r     error = TRUE}
library(randomForest)

system.time(
  rforest.model <- h2o.randomForest(y=y.dep3, x=x.indep3, training_frame = train2.h2o, ntrees = 2000, mtries = 3, max_depth = 4, seed = 1122)
)
```

## Rnotebook prducing error so code not written in chunk
h2o.varimp(rforest.model)




Here I have removed very less important variables to improve accuracy 

so now running final rf model2 and after removing least impt variables we get tr.h2o and te.h2o 
## Rnotebook prducing error so code not written in chunk
# making predictions
predict.reg_rf <- as.data.frame(h2o.predict(rforest.model2,te.h2o))

predict.reg_rf = expm1(predict.reg_rf) 
solution_rf3 <- data.frame(id = te$id,trip_duration =predict.reg_rf$predict)
write.csv(solution_rf3, file = "rf3.csv", row.names = F)







It gave mee an accuracy and score on  kaggle platform came to be Private score 0.57915
 and Public score 0.57909
on kaggle platform.
## Rnotebook prducing error so code not written in chunk
library(caret)
traincv <- createDataPartition(train$trip_duration, p = 0.8, list = FALSE, times = 1)

train1 <- train[traincv,]
valid1 <- train[-traincv,]







I further used a cross validation for a gbm model 

and run folllowing model 

## Rnotebook prducing error so code not written in chunk

library
system.time(
  gbm.model2 <- h2o.gbm(y=y.dep, x=x.indep, training_frame = train1.h2o,
                       validation_frame = valid1.h2o,ntrees = 1000,stopping_rounds = 20,max_depth = 4, learn_rate = 0.3, seed = 1122)




the above model gave an accuracy and RMSLE score on kaggle platform being

  Private score 0.45193
  Public score  0.45516

The highest accuracy aachieved is 0.28 


total submissions - 15 
accuracy - public - 0.45193(RMSLE )
         private   - 0.45516(RMSLE)



## thank you ######



Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).
